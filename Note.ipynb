{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Library Used for Creating various plots -> Matplotlib\n",
    "# Plot suitable for visualizing the trend of numerical variables over time -> Line Plot\n",
    "# Plot suitable for visualizing the distribution of numerical variables -> Histogram\n",
    "# Plot suitable for visualizing the relationship between numerical variables -> Scatter Plot\n",
    "# Plot suitable for visualizing the relationship between numerical and categorical variables -> Bar Plot\n",
    "# Python Library provides high-level interface for creating informative statistical graphics -> Seaborn\n",
    "# Histogram Compares the distribution of a numerical variables\n",
    "# Plot used to display the spread and skewness of a dataset -> Box Plot\n",
    "# And dataset should be numeric for box plot\n",
    "# Python Library used for creating interactive & web-based plots -> Plotly\n",
    "# In a Box Plot vertical Line represents the median of the data\n",
    "# Box Plot is also known as Whisker Plot\n",
    "# Whiskers are basically the lines that extend from the box to the highest and lowest observations\n",
    "# Upper Whisker extends from the top of the box to the largest value that is within 1.5 * IQR from the top of the box (Q3) and you can say upper whisker = Q3 + 1.5 * IQR\n",
    "# Lower Whisker extends from the bottom of the box to the smallest value that is within 1.5 * IQR from the bottom of the box (Q1) and you can say lower whisker = Q1 - 1.5 * IQR\n",
    "# We should use logaritmic scale when the data is skewed (data has an exponential relationship)\n",
    "# Plot used to visualize the distribution of a single Numerical Variable -> Histogram\n",
    "# Pie charts are difficult to interpret accurately when there are too many categories in the dataset\n",
    "# Hue in Seaborn is used to differentiate the categories in the dataset . Color the data points based on a categorical variable\n",
    "# Components of a well-labeled plot -> Title, X-axis Label, Y-axis Label, Legend, Data Points\n",
    "# Alpha -> Transparency of the data points in the plot\n",
    "# Benefits of using Seaborn over Matplotlib -> Seaborn is built on top of Matplotlib, Seaborn has a more visually appealing plotting style than Matplotlib, Seaborn has built-in themes for styling Matplotlib graphics, Seaborn has functions for visualizing univariate and bivariate distributions, Seaborn has functions for visualizing linear regression models, Seaborn has functions for visualizing matrices of data\n",
    "# Seaborn automatically chooses the color palettes for the plots but matplotlib requires the user to specify the colors for the plots\n",
    "# plt.plot()  -> creates a line plot\n",
    "# plt.scatter() -> creates a scatter plot\n",
    "# plt.bar() -> creates a bar plot\n",
    "# plt.hist() -> creates a histogram\n",
    "# plt.boxplot() -> creates a box plot\n",
    "# plt.pie() -> creates a pie chart\n",
    "# plt.show() -> displays the plot\n",
    "# plt.xlabel() -> sets the label for the x-axis\n",
    "# plt.ylabel() -> sets the label for the y-axis\n",
    "# plt.title() -> sets the title of the plot\n",
    "# plt.legend() -> adds a legend to the plot\n",
    "# plt.savefig() -> saves the plot to a file\n",
    "# compairing the distributions or displaying the spread of the data -> Box Plot\n",
    "# Height of the bar represents -> Frequency of the data\n",
    "# bin in histogram -> Range of values that are grouped together\n",
    "# When displaying categorical data -> use Bar Plot instead of Line Plot\n",
    "# Length of the box in Box Plot represents -> IQR (Interquartile Range)\n",
    "# Interpolating missing values based on neighboring data points -> means filling in the missing values with the average of the neighboring data points\n",
    "# data-ink ratio -> emphasizes the importance of minimizing non-data ink to avoid cluttering in visualizations\n",
    "# Use logaritmic scale on an axis -> when the data spans several orders of magnitude meaning the data has an exponential relationship\n",
    "# Legend -> identify different data series or categories in the plot\n",
    "# Voilin plot -> displays the distribution of the data and its probability density\n",
    "# Voilin plot -> combines the features of a box plot and a kernel density plot\n",
    "# Plots cannot be created using matplotlib -> Decision Tree Plot\n",
    "# Library can be used in conjunction with Matplotlib to create statistical graphics -> Seaborn\n",
    "# Bar Plot -> useful for comparing categories of data\n",
    "# Valid color representation in Matplotlib -> RGB, Hexadecimal, Color Name , RGBA , HTML Color Name\n",
    "# Line plot -> Connects data points with a straight line\n",
    "# Overplotting -> occurs when multiple data points are plotted on top of each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary Difference between supervised and unsupervised learning -> Supervised learning requires labeled data while unsupervised learning does not\n",
    "# In ML, features are the input variables that are used to make predictions\n",
    "# In ML, the target variable is the variable that we are trying to predict\n",
    "# Unsupervised learning -> finds hidden patterns or intrinsic structures in the data\n",
    "# Label in ML -> the output variable that we are trying to predict\n",
    "# Examples of supervised learning ->  Image Classification, Spam Detection, House Price Prediction\n",
    "# Examples of unsupervised learning -> Clustering, Anomaly Detection, Dimensionality Reduction, K-means Clustering\n",
    "# Hyperparameters tuning -> is NOT a step in the typical ML workflow\n",
    "# Training Phase purpose -> to adjust the model parameters to minimize the error on the training data\n",
    "# Library used for ML tasks such as classification, regression, and clustering -> Scikit-learn\n",
    "# scikit-learn -> provides tools for ML algorithms and data preprocessing\n",
    "# Supervised learning Algorithm -> Decision Trees , Random Forest , Support Vector Machines\n",
    "# Unsupervised learning Algorithm -> K-means Clustering , Hierarchical Clustering , Principal Component Analysis (PCA)\n",
    "# testing data -> used to evaluate the model's performance on unseen data\n",
    "# Dimensionality Reduction -> reduces the number of features in the dataset and is NOT a type of ML algorithm\n",
    "# Classification Algorithm output -> Labels or categories (discrete values)\n",
    "# Regression Algorithm output -> Continuous values\n",
    "# Clustering Algorithm output -> Groups or clusters\n",
    "# evaluation metric for regression -> Mean Squared Error (MSE)\n",
    "# evaluation metric used for classification tasks -> Accuracy (F-1 Score, Precision, Recall)\n",
    "# evaluation metric used for clustering -> Silhouette Score\n",
    "# Cross-validation purpose -> to prevent overfitting and assess the model's performance\n",
    "# Techniques used to handle missing data -> Imputation , Deletion , Prediction\n",
    "# Imputation : Filling in missing values with a statistical estimate of the missing data based on the other data points in the dataset\n",
    "# Imputation -> Types -> Mean Imputation , Median Imputation , Mode Imputation\n",
    "# Deletion -> Removing rows or columns with missing values from the dataset \n",
    "# Prediction -> Filling in missing values by predicting them using machine learning algorithms \n",
    "# One-hot encoding -> converts categorical variables into numerical format for ML algorithms\n",
    "# Feature scaling purpose -> to standardize or normalize the range of independent variables or features of data\n",
    "# Feature scaling -> Techniques -> Min-Max Scaling , Standardization , Normalization , Robust Scaling\n",
    "# Algorithms used for regression tasks -> Linear Regression , Decision Trees , Random Forest\n",
    "# Algorithms used for classification tasks -> Logistic Regression , Support Vector Machines , K-Nearest Neighbors\n",
    "# Algorithms used for clustering tasks -> K-means Clustering , Hierarchical Clustering , DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "# Dimensionality Reduction -> Techniques -> Principal Component Analysis (PCA) , t-Distributed Stochastic Neighbor Embedding (t-SNE) , Linear Discriminant Analysis (LDA) , Singular Value Decomposition (SVD)\n",
    "# KNN algorithm Drawback -> sensitive to irrelevant features and the scale of the data\n",
    "# Basic Concepts in ML -> Features , Target Variable , Training Data , Testing Data\n",
    "# Testing data -> used to evaluate the performance of a ML model\n",
    "# Training data -> used to train the ML model\n",
    "# Machine Learning Tasks library -> Scikit-learn\n",
    "# Tasks Scikit learn can perform -> Classification , Regression , Clustering\n",
    "# fit method in Scikit-learn -> used to train the ML model\n",
    "# in supervised learning goal -> to make predictions based on labeled data\n",
    "# Drawback of unsupervised learning -> lack of ground truth labels for evaluation\n",
    "# score method in Scikit-learn -> used to evaluate the performance of the ML model\n",
    "# Feature Engineering -> process of transforming raw data into features that can be used to train ML models\n",
    "# Feature Engineering -> Primary goal -> to improve the performance of ML models\n",
    "# Machine Learning Types -> Supervised Learning , Unsupervised Learning , Reinforcement Learning\n",
    "# Supervised Learning -> uses features and labels to make predictions\n",
    "# Data points used for training ML models -> Features\n",
    "# Supervised Learning Algorithms -> Decision Trees , Random Forest , Support Vector Machines (SVM) , Linear Regression\n",
    "# Unsupervised Learning Algorithms -> K-means Clustering , Hierarchical Clustering , Principal Component Analysis (PCA)\n",
    "# algorithms used for classification tasks -> K-Nearest Neighbors (KNN) \n",
    "# algorithms used for regression tasks -> Random Forest\n",
    "# Overfitting -> model memorizes the training data without generalizing well\n",
    "# Hyperparameter -> parameter that controls the learning process and are set before training the model\n",
    "# Cross-validation -> technique used to evaluate the performance of ML models\n",
    "# Hyperparameter tuning -> Techniques -> Grid Search , Random Search , Bayesian Optimization\n",
    "# Preventing Overfitting -> Techniques -> Cross-validation , Regularization , Early Stopping\n",
    "# Cross-validation -> Purpose -> to train the model on multiple subsets of the data and evaluate its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised Learning Algorithms -> Decision Trees , Random Forest , Support Vector Machines (SVM) , Linear Regression\n",
    "# Unsupervised Learning Algorithms -> K-means Clustering , Hierarchical Clustering , Principal Component Analysis (PCA)\n",
    "# Classification Algorithm output -> Labels or categories (discrete values)\n",
    "# Regression Algorithm output -> Continuous values\n",
    "# Supervised Learning -> Primary goal -> to minimize the error between the predicted and actual values\n",
    "# In Linear Regression -> predicted output term -> Hypothesis Function\n",
    "# In Linear Regression -> error term -> Cost Function\n",
    "# fit -> method used to train the model in Scikit-learn\n",
    "# predict -> method used to make predictions using the trained model in Scikit-learn\n",
    "# R-squared -> metric used to evaluate the performance of a regression model\n",
    "# R-squared -> value range -> 0 to 1\n",
    "# R-squared -> value closer to 1 -> better model performance\n",
    "# R-squared -> measures the proportion of the variance in the dependent variable that is predictable from the independent variables\n",
    "# -ve R-squared value -> indicates that the model is worse than a simple average / the model is underfitting the data\n",
    "# Mean Squared Error (MSE) -> metric used to evaluate the performance of a regression model\n",
    "# Evaluate the performance of a regression model when comparing models with different scales of the dependent variable -> RMSE (Root Mean Squared Error)\n",
    "# RMSE -> value range -> 0 to infinity\n",
    "# RMSE -> value closer to 0 -> better model performance\n",
    "# In Linear Regression , Objective during Training phase -> Minimize the sum of squared errors b/w predicted and actual values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median -> Arrange in Ascending Order -> Number of values is odd -> Middle value is Median\n",
    "# Median -> Arrange in Ascending Order -> Number of values is even -> Average of two middle values is Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers \n",
    "\n",
    "\n",
    "# measures of central tendency -> Mean , Median , Mode\n",
    "# measures of variability -> Range , Variance , Standard Deviation\n",
    "# measures of central tendency used for categorical data -> Mode\n",
    "# measures of central tendency used for numerical data -> Mean , Median\n",
    "# measures of central tendency used for dataset with outliers to get a representative value -> Median\n",
    "# measures of central tendency used for dataset with skewed distribution -> Median\n",
    "# measures of central tendency most affected by outliers -> Mean\n",
    "# measures of central tendency most affected by skewed distribution -> Mean\n",
    "# measures of central tendency most resistant to outliers -> Median\n",
    "# Positive Skewness -> Tail on the right side of the distribution\n",
    "# Positively Skewed Distribution -> measures of central tendency -> Mode < Median < Mean\n",
    "# Skewness -> shape of the distribution of data points\n",
    "# Mean -> sensitive to outliers\n",
    "\n",
    "# Variance -> measures the spread of the data points around the mean\n",
    "# Variance -> value range -> 0 to infinity\n",
    "# Variance -> value closer to 0 -> data points are closer to the mean\n",
    "# Variance -> value further from 0 -> data points are more spread out\n",
    "# Standard Deviation -> square root of the variance\n",
    "# Standard Deviation -> value range -> 0 to infinity\n",
    "# Standard Deviation -> value closer to 0 -> data points are closer to the mean\n",
    "# Standard Deviation -> value further from 0 -> data points are more spread out\n",
    "# Standard Deviation -> measures the dispersion of data points around the mean similar to variance\n",
    "# Difference between Variance and Standard Deviation is that -> Standard Deviation is the square root of the variance but both measure the spread of the data points around the mean\n",
    "\n",
    "Statistics\n",
    "# Descriptive Statistics -> primary goal -> to summarize and describe the main features of a dataset\n",
    "\n",
    "Legend\n",
    "# Set Background Color -> set_facecolor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Plotting</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------linestyle-------------------------------------------------\n",
    "\n",
    "# used to change the style of a plotted line\n",
    "\n",
    "# -----------------------------------Box Plot-------------------------------------------------\n",
    "\n",
    "# Also known as Whisker Plot\n",
    "# Horizontal box plot : plt.boxplot(data, vert = False)\n",
    "# vert means vertical box plot\n",
    "\n",
    "# -----------------------------------Plotting a Horizontal Line---------------------------------\n",
    "\n",
    "# axhline()\n",
    "\n",
    "# -----------------------------------Plotting a Vertical Line-----------------------------------\n",
    "\n",
    "# axvline()\n",
    "\n",
    "# -----------------------------------Legend-----------------------------------------------------\n",
    "\n",
    "# Set background color -> set_facecolor()\n",
    "\n",
    "# -----------------------------------Matplotlib--------------------------------------------------\n",
    "\n",
    "# designed to function in terms of usability : MATLAB\n",
    "# to check version : __version__\n",
    "# default color of the plot : blue\n",
    "\n",
    "# -----------------------------------Histogram--------------------------------------------------\n",
    "\n",
    "# Ploting : data.plot(type = 'hist', edgecolor = 'black')\n",
    "# edgecolor : color of the edges of the bars\n",
    "\n",
    "# -----------------------------------Plotting without line---------------------------------------\n",
    "\n",
    "# plt.plot(x, y, 'o')\n",
    "# 'o' : marker\n",
    "\n",
    "# -----------------------------------Exploratory Graphs in Data Analysis-------------------------\n",
    "\n",
    "# typically made very quickly\n",
    "\n",
    "# -----------------------------------Seaborn-----------------------------------------------------\n",
    "\n",
    "# type of plots available : Line Plot, Bar Plot, Histogram, Box Plot, Violin Plot, Heatmap, Pairplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training & Testing Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML algorithms build a model based on sample data -> known as training data\n",
    "# ML algorithms make predictions on new data -> known as testing data\n",
    "# ML algorithms learn patterns from the training data to make predictions on the testing data -> known as generalization\n",
    "# ML algorithms are trained on labeled data -> known as supervised learning\n",
    "# ML algorithms are trained on unlabeled data -> known as unsupervised learning\n",
    "# ML algorithms are trained on a mix of labeled and unlabeled data -> known as semi-supervised learning\n",
    "# ML algorithms are trained on rewards or penalties -> known as reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Regression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------Uses -------------------------------------------------------\n",
    "\n",
    "# Regression is used for prediction of continuous values , for interpretation , related input to outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Test Set</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set is used to test the accuracy of the hypothesis generated by the learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Supervised Learning Algorithms</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------Applications------------------------------------------------\n",
    "\n",
    "# Spam Detection , Pattern Detection , Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pandas</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------Index Values------------------------------------------------\n",
    "\n",
    "# must be unique and hashable\n",
    "# hashable means that the value does not change over time or you can say immutable\n",
    "\n",
    "# -----------------------------------Series------------------------------------------------------\n",
    "\n",
    "# Series Size : Immutable , Series Values : Mutable\n",
    "# Parameters : data , index , dtype , copy\n",
    "# Data labels :  Numeric (default) which starts from 0\n",
    "# Series : 1D labeled array capable of holding data of any type\n",
    "# Create: pd.Series(data = [values], index = [labels])\n",
    "# Access : series['label'] or series[0]\n",
    "\n",
    "# -----------------------------------DataFrames--------------------------------------------------\n",
    "\n",
    "# Parameters : data , index , columns , dtype , copy\n",
    "# Can have heterogeneous data types\n",
    "# Size : Mutable , Values : Mutable\n",
    "# Rename a col : df.rename(columns = {'old_name' : 'new_name'})\n",
    "# Apply a function to each element : using apply() method\n",
    "\n",
    "# -----------------------------------resample()--------------------------------------------------\n",
    "\n",
    "# changes the frequency of the time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Numpy Array</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------Requirements------------------------------------------------\n",
    "\n",
    "# NumPy array requires all elements to be of the same data type (homogeneous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Attributes</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------Types-------------------------------------------------------\n",
    "\n",
    "# Nominal , Ordinal , Spacial , Temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logistic Regression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------Likelihood function-----------------------------------------\n",
    "\n",
    "# Maximized / Optimized : By Maximizing the log-likelihood function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Linear Regression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------Output Variable---------------------------------------------\n",
    "\n",
    "# If we change the input variable by 1 unit , the output variable changes by the slope\n",
    "\n",
    "# -----------------------------------Size of Training Data Increase------------------------------\n",
    "\n",
    "# Bias Increases , Variance Decreases\n",
    "\n",
    "# -----------------------------------Output Range of sigmoid function--------------------------\n",
    "\n",
    "# 0 to 1\n",
    "\n",
    "# -----------------------------------Relationship between Residuals and Predicted Values--------\n",
    "\n",
    "# If there is a relationship between residuals and predicted values , then the model is not capturing the underlying patterns in the data\n",
    "\n",
    "# -----------------------------------If Residuals Exhibit Heteroscedasticity----------------------------\n",
    "\n",
    "# means residuals have a varying variance across different levels of the independent values\n",
    "\n",
    "# -----------------------------------Difference between Residual and predicted Value ----------------\n",
    "\n",
    "# The residual represents the difference between the predicted and actual values, while the predicted value is the value estimated by the regression equation\n",
    "\n",
    "# -----------------------------------Triggers the discovery of line of best fit or Regression Line----------------\n",
    "\n",
    "# Sum of Square of Residuals (∑(y - h(x))^2) is minimum\n",
    "\n",
    "# -----------------------------------In practice, Line of best fit or regression line is found when ------------------------\n",
    "\n",
    "#  Sum of the square of residuals ( ∑ (Y-h(X))2) is minimum\n",
    "\n",
    "# -----------------------------------Name of Line that best fits the data points------------------------\n",
    "\n",
    "# Regression Line\n",
    "\n",
    "# -----------------------------------How many variables are required to represent a linear regression model ------------------------\n",
    "\n",
    "# 2 variables\n",
    "\n",
    "# -----------------------------------How many coefficients must estimate in a simple linear regression model with one independent variable ------------------------\n",
    "\n",
    "# 2 coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Decision Tree</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------Handles Missing Values--------------------------------------\n",
    "\n",
    "# By using surrogate splits to make decisions based on available data\n",
    "\n",
    "# -----------------------------------Handle Categorical Variables with multiple levels------------\n",
    "\n",
    "# By splitting the variable into binary dummy variables -> One-hot encoding\n",
    "\n",
    "# -----------------------------------Calculating Information Gain---------------------------------\n",
    "\n",
    "# By measuring the decrease in impurity after a dataset is split on an attribute\n",
    "\n",
    "# -----------------------------------criterion employed to gauge impurity----------------------------\n",
    "\n",
    "# Gini impurity , Entropy , Misclassification  error\n",
    "# Measure the impurity in decision trees is also same.\n",
    "\n",
    "# -----------------------------------Techniques used to prevent overfitting in decision trees-------\n",
    "\n",
    "# Pruning , Feature Selection , Regularization , Cross-validation\n",
    "\n",
    "# -----------------------------------Signify in ml -----------------------------------------------\n",
    "\n",
    "# A flowchart-like tree structure where each internal node represents a decision based on a feature, each branch represents the outcome of the decision, and each leaf node represents the final decision or outcome\n",
    "\n",
    "# -----------------------------------Decision stump------------------------------------------------\n",
    "\n",
    "# A decision tree with single decision node and two leaf nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Random Forest <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------Handles Noisy Features---------------------------------------\n",
    "\n",
    "# By randomlly selecting features for each tree\n",
    "\n",
    "# -----------------------------------Handles Missing Values--------------------------------------\n",
    "\n",
    "# By ignoring observations with missing values\n",
    "\n",
    "# -----------------------------------Address Class Imbalance-------------------------------------\n",
    "\n",
    "# By adjusting the class weights during training\n",
    "\n",
    "# -----------------------------------Used to evaluate the performance of Random Forest------------\n",
    "\n",
    "# Accuracy , Precision , Recall , F1 Score\n",
    "\n",
    "# -----------------------------------Random Forest entails----------------------------------------\n",
    "\n",
    "# A type of supervised learning algorithm\n",
    "\n",
    "# -----------------------------------Forest in Random Forest-------------------------------------\n",
    "\n",
    "# Collection of decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>R-Squared</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------Interpretation ------------------------------------------------\n",
    "\n",
    "# R-squared value of 1 -> all the variance in the dependent variable is explained by the independent variables\n",
    "# R-squared value of 0.8 -> 80% of the variance in the dependent variable is explained by the independent variables\n",
    "\n",
    "# -----------------------------------Range --------------------------------------------------------\n",
    "\n",
    "# 0 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Metric</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------Appropriate for multiclass classification problems------------\n",
    "\n",
    "# Precision , Recall , F1 Score , Confusion Matrix , Classification Report\n",
    "\n",
    "# -----------------------------------Evaluation Metric for Logistic Regression---------------------\n",
    "\n",
    "# Accuracy, Precision, Recall, F1 Score, ROC-AUC\n",
    "\n",
    "# -----------------------------------For Binary Classification Problems----------------------------\n",
    "\n",
    "# Same as for Logistic Regression\n",
    "\n",
    "# -----------------------------------Evaluate performance of a binary classifier at various threshold levels ----------------\n",
    "\n",
    "# ROC - AUC\n",
    "\n",
    "# -----------------------------------Disadvantage of using Accuracy as an evaluation metric-------------------------------\n",
    "\n",
    "# Sensitivity to class imbalance\n",
    "\n",
    "# -----------------------------------Evaluation Metric for dealing with imbalanced datasets------------------------\n",
    "\n",
    "# F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Accuracy</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------Statements-----------------------------------------------------\n",
    "\n",
    "# Accuracy considers both false positives and false negatives\n",
    "# Accuracy is suitable for balanced datasets but not for imbalanced datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Mean Absolute Error</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------Range----------------------------------------------------------\n",
    "\n",
    "# 0 to infinity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Matthews Correlation Coefficient (MCC)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------Calculateed----------------------------------------------------\n",
    "\n",
    "# (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neural Network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------Softmax Function------------------------------------------------\n",
    "\n",
    "# Purpose : Convert raw scores into probabilities\n",
    "\n",
    "# -----------------------------------Type of Neural Network best suited for sequential data----------------------------------------\n",
    "\n",
    "# Recurrent Neural Network (RNN)\n",
    "\n",
    "# -----------------------------------Type of Neural Network best suited for time series prediction----------------------------------------\n",
    "\n",
    "# Recurrent Neural Network (RNN)\n",
    "\n",
    "# -----------------------------------Type of Neural Network best suited for image recognition tasks----------------------------------------\n",
    "\n",
    "# Convolutional Neural Network (CNN)\n",
    "\n",
    "# -----------------------------------Bias in Neural Networks----------------------------------------\n",
    "\n",
    "# Main function : Adds flexibility to the model\n",
    "\n",
    "# -----------------------------------Role of Learning rate in training a neural network -------------------------------------------------\n",
    "\n",
    "# Controls the size of weight updates\n",
    "\n",
    "# -----------------------------------Process of Adjusting weights and biases in neural networks to minimize errors------------------------\n",
    "\n",
    "# Backpropagation\n",
    "\n",
    "# -----------------------------------Type of Neural Network Layers--------------------------------------------------------------------------\n",
    "\n",
    "# Dropout Layer, Activation Layer, Pooling Layer, Convolutional Layer, Dense Layer\n",
    "# Normalization Layer is not a type of neural network layer\n",
    "\n",
    "# ----------------------------------- layer in a neural network performs the element-wise activation function ------------------------------\n",
    "\n",
    "# Activation Layer\n",
    "\n",
    "# ----------------------------------- Difference between perceptron and multi-layer perceptron ------------------------------------------------\n",
    "\n",
    "# Perceptron has 1 layer while multi-layer perceptron has multiple layers\n",
    "\n",
    "# -----------------------------------PCA -> unsupervised and LDA -> supervised------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Principal Component Analysis (PCA)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------Primary Goal---------------------------------------------------\n",
    "\n",
    "# Dimentionality Reduction\n",
    "\n",
    "# Can be applied to both numerical and categorical data\n",
    "\n",
    "# -----------------------------------Outliers on PCA-------------------------------------------------\n",
    "\n",
    "# Outliers may influence principal component directions\n",
    "\n",
    "# ------------------------------------EigenVector in PCA --------------------------------------------\n",
    "\n",
    "# Weighted combination of original features\n",
    "\n",
    "# -----------------------------------matrix is diagonalized in PCA to obtain eigenvalues and eigenvectors---------------------\n",
    "\n",
    "# Covariance matrix\n",
    "\n",
    "# ------------------------------------computational complexity of PCA --------------------------------------------\n",
    "\n",
    "# O(n^2)\n",
    "\n",
    "# ------------------------------------interpretation of negative loadings in PCA--------------------------------\n",
    "\n",
    "# Negative correlation between features\n",
    "\n",
    "# --------------------------------------amount of variance explained by each principal component calculated----------------------------\n",
    "\n",
    "# Cumulative percentage of explained variance\n",
    "\n",
    "# --------------------------------------Purpose of scaling the data before performing PCA--------------------------------------------\n",
    "\n",
    "# Prevents numerical instabilites\n",
    "\n",
    "# ----------------------------------------Handles Missing Values -------------------------------\n",
    "\n",
    "# Imputes the missing values\n",
    "\n",
    "# ----------------------------------------Eigen Values Significance ---------------------------------------\n",
    "\n",
    "# Measure of variance captured by each principal component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>K-means Clustering</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- Not Limitation----------------------------------------------------\n",
    "\n",
    "# Prone to Overfitting\n",
    "\n",
    "# -----------------------------------Step responsible for updating the cluster membership of data points---------------------------\n",
    "\n",
    "# Assignment\n",
    "\n",
    "# -----------------------------------Step can be computationally expensive for large datasets -------------------------------------\n",
    "\n",
    "# Update centroids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
